{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [SC57 - Working with big, multi-dimensional geoscientific datasets in Python: a tutorial introduction to xarray](http://meetingorganizer.copernicus.org/EGU2017/session/25651)  \n",
    "  \n",
    "  \n",
    "Original notebook by [Stephan Hoyer](http://stephanhoyer.com), Rossbypalooza, 2016.  \n",
    "Modified by Edward Byers, Matthew Gidden and [Fabien Maussion](http://fabienmaussion.info/) for EGU General Assembly 2017, Vienna, Austria\n",
    "Modified by C. Gentemann for GHRSST Science Team Tutorial 2019, Rome, Italy\n",
    "  \n",
    "  Sunday, 31 May 2019, 9:00 - 2:00  Hotel Flora  \n",
    "  \n",
    "  \n",
    "**Convenors**\n",
    "* [Dr Chelle Gentemann](mailto:gentemann@esr.org)    - Earth and Space Research, USA\n",
    "* [Dr Marisol Garcia-Reyes](mailto:marisolgr@faralloninstitute.org)  - Farallon Institute, USA \n",
    "-------------\n",
    "![dataset-diagram-logo.png](attachment:dataset-diagram-logo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of this tutorial\n",
    "\n",
    "1. Opening data\n",
    "1. Collocating satellite data with a cruise dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Key features of `xarray`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "## Import python packages\n",
    "\n",
    "You are going to want numpy, pandas, matplotlib.pyplot and xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore') # filter some warning messages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "#for search capabilites import podaacpy\n",
    "import podaac.podaac as podaac\n",
    "import podaac.podaac_utils as putil\n",
    "# then create an instance of the Podaac class\n",
    "p = podaac.Podaac()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A nice cartopy tutorial is [here](http://earthpy.org/tag/visualization.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocate a Saildrone cruise with AVHRR SST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's open 2 months of 0.2 km AVHRR OI SST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xarray`can open multiple files at once using string pattern matching.  \n",
    "  \n",
    "  In this case we open all the files that match our `filestr`, i.e. all the files for the 2080s. \n",
    "  \n",
    "  Each of these files (compressed) is approximately 800 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'PODAAC-GHGMR-4FJ04'  #MUR SST looked up on podaac website\n",
    "start_time='2018-04-11T00:00:00Z'\n",
    "end_time='2018-06-11T23:59:59Z'\n",
    "gresult = p.granule_search(dataset_id=dataset_id,\n",
    "                           start_time=start_time,\n",
    "                           end_time=end_time,\n",
    "                           items_per_page='100')\n",
    "urls = putil.PodaacUtils.mine_opendap_urls_from_granule_search(gresult)\n",
    "urls = [w[:-5] for w in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sst = xr.open_mfdataset(urls,coords='minimal')\n",
    "ds_sst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How big is all this data uncompressed? Will it fit into memory?\n",
    "Use `.nbytes` / 1e9  to convert it into gigabytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sst.nbytes / 1e9  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocating Saildrone cruise data with MUR SSTs \n",
    "\n",
    "* read in the Saildrone data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://podaac-opendap.jpl.nasa.gov/opendap/hyrax/allData/insitu/L2/saildrone/Baja/saildrone-gen_4-baja_2018-sd1002-20180411T180000-20180611T055959-1_minutes-v1.nc'\n",
    "ds_usv = xr.open_dataset(url)\n",
    "ds_usv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NCEI trajectory format uses 'obs' as the coordinate.  This is an example of an 'older' style of data formatting that doesn't really mesh well with modern software capabilities. \n",
    "\n",
    "* So, let's change that by using [.swap_dims](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.swap_dims.html) to change the coordinate from `obs` to `time`\n",
    "* Another thing, `latitude` and `longitude` are just long and annoying, lets [.rename](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.rename.html) them to `lat` and `lon`\n",
    "\n",
    "* Finally, the first and last part of the cruise the USV is being towed, so let's only include data from `2018-04-12T02` to `2018-06-10T18`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_usv = ds_usv.isel(trajectory=0).swap_dims({'obs':'time'}).rename({'longitude':'lon','latitude':'lat'})\n",
    "ds_usv_subset = ds_usv.sel(time=slice('2018-04-12T02','2018-06-10T18')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xarray interpolation won't run on chunked dimensions.  \n",
    "1. First let's subset the data to make it smaller to deal with by using the cruise lat/lons\n",
    "    * Find the max/min of the lat/lon using `.lon.min().data`\n",
    "\n",
    "1. Now load the data into memory (de-Dask-ify) it using `.load()`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 from above\n",
    "print('min max lat lon:', ds_usv_subset.lon.min().data,ds_usv_subset.lon.max().data,ds_usv_subset.lat.min().data,ds_usv_subset.lat.max().data)\n",
    "subset = ds_sst.sel(lon=slice(ds_usv_subset.lon.min().data,ds_usv_subset.lon.max().data),\n",
    "                    lat=slice(ds_usv_subset.lat.min().data,ds_usv_subset.lat.max().data))\n",
    "#Step 2 from above\n",
    "subset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocate USV data with MUR data\n",
    "There are different options when you interpolate.  First, let's just do a linear interpolation using [.interp()](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.interp.html#xarray.Dataset.interp)\n",
    "\n",
    "`Dataset.interp(coords=None, method='linear', assume_sorted=False, kwargs={}, **coords_kwargs))`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_collocated = subset.interp(lat=ds_usv_subset.lat,lon=ds_usv_subset.lon,time=ds_usv_subset.time,method='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocate USV data with MUR data\n",
    "There are different options when you interpolate.  First, let's just do a nearest point rather than interpolate the data\n",
    "`method = 'nearest'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_collocated_nearest = subset.interp(lat=ds_usv_subset.lat,lon=ds_usv_subset.lon,time=ds_usv_subset.time,method='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, calculate the different in SSTs and print the [.mean()](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.mean.html#xarray.DataArray.mean) and [.std()](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.std.html#xarray.DataArray.std)\n",
    "For the satellite data we need to use `sst` and for the USV data we need to use `TEMP_CTD_MEAN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dif = ds_collocated_nearest.sst-ds_usv.TEMP_CTD_MEAN\n",
    "print('mean difference = ',dif.mean().data)\n",
    "print('STD = ',dif.std().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A larger STD that isn't reflective of uncertainty in the observation\n",
    "The collocation above will result in multiple USV data points matched with a single satellite\n",
    "observation.    The USV is sampling every 1 min and approximately few meters, while the satellite\n",
    "is an average over a footprint that is interpolated onto a daily mean map.  While calculating the mean would results in a valid mean, the STD would be higher and consist of a component that reflects the uncertainty of the USV and the satellite and a component that reflects the natural variability in the region that is sampled by the USV\n",
    "\n",
    "Below we use the 'nearest' collocation results to identify when multiple USV data are collcated to\n",
    "a single satellite observation.\n",
    "This code goes through the data and creates averages of the USV data that match the single CCMP collocated value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ilen,index = ds_collocated_nearest.dims['time'],0\n",
    "ds_tem = ds_collocated_nearest.copy(deep=True)\n",
    "duu, duv1, duv2, dlat, dlon, dut = [],[],[],[],[],np.empty((),dtype='datetime64')\n",
    "while index <= ilen-2:\n",
    "    index += 1\n",
    "    if np.isnan(ds_collocated_nearest.analysed_sst[index]):\n",
    "        continue\n",
    "    if np.isnan(ds_tem.analysed_sst[index]):\n",
    "        continue\n",
    "   # print(index, ilen)\n",
    "    iend = index + 1000\n",
    "    if iend > ilen-1:\n",
    "        iend = ilen-1\n",
    "    ds_tem_subset = ds_tem.analysed_sst[index:iend]\n",
    "    ds_usv_subset2sst = ds_usv_subset.TEMP_CTD_MEAN[index:iend]\n",
    "    ds_usv_subset2uwnd = ds_usv_subset.UWND_MEAN[index:iend]\n",
    "    ds_usv_subset2vwnd = ds_usv_subset.VWND_MEAN[index:iend]\n",
    "    ds_usv_subset2lat = ds_usv_subset.lat[index:iend]\n",
    "    ds_usv_subset2lon = ds_usv_subset.lon[index:iend]\n",
    "    ds_usv_subset2time = ds_usv_subset.time[index:iend]\n",
    "    cond = ((ds_tem_subset==ds_collocated_nearest.analysed_sst[index]))\n",
    "    notcond = np.logical_not(cond)\n",
    "    #cond = ((ds_tem.analysed_sst==ds_collocated_nearest.analysed_sst[index]))\n",
    "    #notcond = np.logical_not(cond)\n",
    "    masked = ds_tem_subset.where(cond)\n",
    "    if cond.sum().data==0:  #don't do if data not found\n",
    "        continue\n",
    "    if cond.sum().data>800:\n",
    "        print(cond.sum().data,index,ds_tem.time[index].data)\n",
    "    masked_usvsst = ds_usv_subset2sst.where(cond,drop=True)\n",
    "    masked_usvuwnd = ds_usv_subset2uwnd.where(cond,drop=True)\n",
    "    masked_usvvwnd = ds_usv_subset2vwnd.where(cond,drop=True)\n",
    "    masked_usvlat = ds_usv_subset2lat.where(cond,drop=True)\n",
    "    masked_usvlon = ds_usv_subset2lon.where(cond,drop=True)\n",
    "    masked_usvtime = ds_usv_subset2time.where(cond,drop=True)\n",
    "    duu=np.append(duu,masked_usvsst.mean().data)\n",
    "    duv1=np.append(duv1,masked_usvuwnd.mean().data)\n",
    "    duv2=np.append(duv2,masked_usvvwnd.mean().data)\n",
    "    dlat=np.append(dlat,masked_usvlat.mean().data)\n",
    "    dlon=np.append(dlon,masked_usvlon.mean().data)\n",
    "    tdif = masked_usvtime[-1].data-masked_usvtime[0].data\n",
    "    mtime=masked_usvtime[0].data+np.timedelta64(tdif/2,'ns')\n",
    "    if mtime>dut.max():\n",
    "        print(index,masked_usvtime[0].data,(masked_usvtime[-1].data-masked_usvtime[0].data)/1e9)\n",
    "    dut=np.append(dut,mtime)\n",
    "    ds_tem.analysed_sst[index:iend]=ds_tem.analysed_sst.where(notcond)\n",
    "#    ds_tem=ds_tem.where(notcond,np.nan)  #masked used values by setting to nan\n",
    "dut2 = dut[1:]  #remove first data point which is a repeat from what array defined    \n",
    "ds_new=xr.Dataset(data_vars={'sst_usv': ('time',duu),'uwnd_usv': ('time',duv1),'vwnd_usv': ('time',duv2),\n",
    "                             'lon': ('time',dlon),\n",
    "                             'lat': ('time',dlat)},\n",
    "                  coords={'time':dut2})\n",
    "ds_new.to_netcdf('F:/data/cruise_data/saildrone/baja-2018/mur_downsampled_usv_data2.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_collocated_nearest.to_netcdf('./data/new file.nc')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
